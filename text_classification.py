# -*- coding: utf-8 -*-
"""Text_classification (Prac9).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hBj9U3DWeGBWaVui82Q26B5VWowEX0qR

# Practical 9
## Perform text classification
"""

# Sentiment Analysis Dataset Inspector with Visualizations
# --------------------------------------------------------
# 1. Loads IMDB dataset from Hugging Face
# 2. Checks for dataset problems (missing, duplicates, imbalance, outliers)
# 3. Trains TF-IDF + Logistic Regression
# 4. Plots class distribution & text length histogram
# 5. Reports test accuracy & shows flagged mislabeled reviews
# --------------------------------------------------------

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datasets import load_dataset
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# 1. Load dataset
dataset = load_dataset("imdb")
df = pd.DataFrame(dataset["train"])   # use training set
df["label"] = df["label"].map({0: "negative", 1: "positive"})

print("Dataset shape:", df.shape)
print(df["label"].value_counts())

# 2. Problem detection
report = {}
report["num_rows"] = df.shape[0]
report["missing_text"] = df["text"].isna().sum()
report["missing_labels"] = df["label"].isna().sum()
report["duplicate_texts"] = df.duplicated(subset=["text"]).sum()
report["label_counts"] = df["label"].value_counts().to_dict()

lengths = df["text"].str.len()
report["text_min_len"] = int(lengths.min())
report["text_max_len"] = int(lengths.max())
report["text_mean_len"] = float(lengths.mean())
report["text_median_len"] = float(lengths.median())

vals = np.array(list(report["label_counts"].values()), dtype=float)
imbalance_ratio = vals.max() / (vals.sum() - vals.max() + 1e-9)
report["imbalance_ratio"] = imbalance_ratio

# 3. Train/Test split
X_train, X_test, y_train, y_test = train_test_split(
    df["text"], df["label"], test_size=0.25, random_state=42, stratify=df["label"]
)

vec = TfidfVectorizer(max_features=20000, ngram_range=(1,2))
Xtr = vec.fit_transform(X_train)
Xte = vec.transform(X_test)

clf = LogisticRegression(max_iter=1000)
clf.fit(Xtr, y_train)
preds = clf.predict(Xte)
probs = clf.predict_proba(Xte)

acc = accuracy_score(y_test, preds)
print("\n=== TEST PERFORMANCE ===")
print("Test Accuracy:", acc)
print(classification_report(y_test, preds))

# 4. Flag possible mislabeled rows
max_probs = probs.max(axis=1)
disagree_mask = (preds != y_test) & (max_probs > 0.9)
flagged = pd.DataFrame({
    "text": X_test[disagree_mask].values,
    "true_label": y_test[disagree_mask].values,
    "pred_label": preds[disagree_mask],
    "pred_confidence": max_probs[disagree_mask]
})

# 5. Visualizations
plt.figure(figsize=(12,5))

# Class distribution
plt.subplot(1,2,1)
df["label"].value_counts().plot(kind="bar", color=["red","green"])
plt.title("Class Distribution (IMDB Reviews)")
plt.xlabel("Label")
plt.ylabel("Count")

# Review length histogram
plt.subplot(1,2,2)
plt.hist(lengths, bins=50, color="skyblue", edgecolor="black")
plt.title("Review Length Distribution")
plt.xlabel("Length (# characters)")
plt.ylabel("Frequency")

plt.tight_layout()
plt.show()

# 6. Dataset Report
print("\n=== DATASET REPORT ===")
for k, v in report.items():
    print(f"{k}: {v}")

print("\n=== POSSIBLE PROBLEMS ===")
if report["missing_text"] > 0: print("- Missing text entries")
if report["missing_labels"] > 0: print("- Missing labels")
if report["duplicate_texts"] > 0: print("- Duplicate reviews detected")
if imbalance_ratio > 3: print("- Severe class imbalance detected")
if flagged.shape[0] > 0:
    print(f"- {flagged.shape[0]} reviews flagged as possible mislabels")
else:
    print("- No obvious mislabeled examples found")

print("\nSample flagged rows (if any):")
print(flagged.head())

# ---- Test a custom query ----
def test_query(text):
    Xq = vec.transform([text])            # Convert query into TF-IDF features
    pred = clf.predict(Xq)[0]             # Predicted label
    prob = clf.predict_proba(Xq).max()    # Confidence score
    return pred, prob

# Examples
queries = [
    "I really loved this movie, it was fantastic!",
    # "This was the worst film I have ever seen.",
    # "The plot was boring but the acting was decent."
    "The movie was nice but the acting scene was feel like overacting",
    "Very good movie that you can not save your money by wasting on it"
]

for q in queries:
    label, conf = test_query(q)
    print(f"Query: {q}\n → Prediction: {label} (confidence: {conf:.2f})\n")

"""## Part 2

# Task
Train an SVM classification model on the combined data from "BiodiversityConservationDataset.csv", "ClimateChangeDataset.csv", "SocialJusticeDataset.csv", "SpaceExplorationDataset.csv", and "TechnologyImpactDataset.csv". Then, create a function to predict the sentiment of user-provided sentences using the trained model and display the prediction and confidence score in the format "Query: [user sentence] → Prediction: [predicted label] (confidence: [confidence score])".

## Load multiple datasets

### Subtask:
Load the five specified datasets into pandas DataFrames and combine them.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Define file paths
file_paths = [
    "/content/drive/My Drive/Colab Notebooks/BiodiversityConservationDataset.csv",
    "/content/drive/My Drive/Colab Notebooks/ClimateChangeDataset.csv",
    "/content/drive/My Drive/Colab Notebooks/SocialJusticeDataset.csv",
    "/content/drive/My Drive/Colab Notebooks/SpaceExplorationDataset.csv",
    "/content/drive/My Drive/Colab Notebooks/TechnologyImpactDataset.csv"
]

# Load and combine datasets
all_data = []
for file_path in file_paths:
    try:
        df = pd.read_csv(file_path)
        all_data.append(df)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
    except Exception as e:
        print(f"An error occurred while reading {file_path}: {e}")

combined_df = pd.concat(all_data, ignore_index=True)

print("Combined Dataset Shape:", combined_df.shape)
display(combined_df.head())

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
import numpy as np
import matplotlib.pyplot as plt # Import matplotlib

# 2. Preprocess the data (assuming 'sentence' and 'sentiment' columns exist)
# Handle missing values
combined_df.dropna(subset=['sentence', 'sentiment'], inplace=True)

# Handle duplicates
combined_df.drop_duplicates(subset=['sentence'], inplace=True)

# Ensure consistent labeling (assuming 'positive' and 'negative' labels)
# If you have other labels, you might need to adjust this
combined_df = combined_df[combined_df['sentiment'].isin(['positive', 'negative'])]

print("\n--- Preprocessing Report ---")
print("Shape after dropping missing values and duplicates:", combined_df.shape)
print("Value counts after filtering labels:\n", combined_df['sentiment'].value_counts())


# 3. Split data
X = combined_df['sentence']
y = combined_df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

print("\n--- Data Split ---")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)


# 4. Vectorize text data
# Using the same parameters as the previous example for consistency
vec = TfidfVectorizer(max_features=20000, ngram_range=(1,2))
Xtr = vec.fit_transform(X_train)
Xte = vec.transform(X_test)

print("\n--- Vectorization ---")
print("Xtr shape:", Xtr.shape)
print("Xte shape:", Xte.shape)


# 5. Train an SVM model
# Using a linear kernel for simplicity, can be adjusted
svm_clf = SVC(kernel='linear', probability=True, random_state=42)
svm_clf.fit(Xtr, y_train)

print("\n--- Model Training Complete ---")


# 6. Evaluate the model
preds = svm_clf.predict(Xte)
probs = svm_clf.predict_proba(Xte)

acc = accuracy_score(y_test, preds)
print("\n=== TEST PERFORMANCE (SVM) ===")
print("Test Accuracy:", acc)
print(classification_report(y_test, preds))

# 7. Visualizations (Added)
plt.figure(figsize=(12, 5))

# Class distribution
plt.subplot(1, 2, 1)
combined_df['sentiment'].value_counts().plot(kind='bar', color=['green', 'red'])
plt.title('Class Distribution (Combined Dataset)')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(rotation=0) # Keep labels horizontal


# Sentence length histogram
plt.subplot(1, 2, 2)
lengths = combined_df['sentence'].str.len() # Calculate lengths for combined data
plt.hist(lengths, bins=20, color='skyblue', edgecolor='black') # Adjusted bins for smaller dataset
plt.title('Sentence Length Distribution (Combined Dataset)')
plt.xlabel('Length (# characters)')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()


# 8. Implement query testing
def test_query_svm(text):
    Xq = vec.transform([text])            # Convert query into TF-IDF features
    pred = svm_clf.predict(Xq)[0]         # Predicted label
    # Get probability for the predicted class
    prob = svm_clf.predict_proba(Xq)
    if pred == 'positive':
        conf = prob[0][svm_clf.classes_.tolist().index('positive')]
    else:
        conf = prob[0][svm_clf.classes_.tolist().index('negative')]
    return pred, conf

print("\n--- Query Testing ---")
# Examples
queries = [
    "This is a fantastic development for humanity.",
    "I am very concerned about the environmental impact.",
    "The new technology is neither good nor bad.",
    "This is the worst possible outcome.",
    "the food is not bad"
]

for q in queries:
    label, conf = test_query_svm(q)
    print(f"Query: {q}\n → Prediction: {label} (confidence: {conf:.2f})\n")